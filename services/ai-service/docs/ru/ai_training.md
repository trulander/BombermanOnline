# Обучение моделей AI

## Концепция

Сервис использует обучение с подкреплением (Reinforcement Learning) для создания моделей поведения AI. Процесс обучения происходит в кастомной среде, симулирующей правила и механику игры Bomberman.

## Среда обучения (`BombermanEnv`)

*   **Основа**: `gymnasium.Env`
*   **Назначение**: Предоставляет стандартизированный интерфейс для взаимодействия с RL-агентом.
*   **Ключевые компоненты**:
    *   **State (Состояние)**: Представление игрового мира в виде, понятном для модели (обычно многомерный массив `numpy`). Оно включает положение стен, игроков, врагов, бомб и бонусов.
    *   **Action Space (Пространство действий)**: Набор всех возможных действий, которые может совершить агент (движение вверх/вниз/влево/вправо, установка бомбы, ничего не делать).
    *   **Reward (Вознаграждение)**: Числовая оценка, которую агент получает за свои действия. Правильно настроенная функция вознаграждения — ключ к обучению эффективного поведения.
        *   *Позитивное вознаграждение*: за уничтожение врага, сбор бонуса, выживание.
        *   *Негативное вознаграждение (штраф)*: за смерть, потерю очков здоровья, бездействие.
    *   **`step()` метод**: Принимает действие от агента, обновляет состояние среды и возвращает новое состояние, вознаграждение и флаг окончания эпизода.
    *   **`reset()` метод**: Сбрасывает среду в начальное состояние для начала нового эпизода обучения.

## Процесс тренировки (`Trainer`)

1.  **Инициализация**: Создается экземпляр среды `BombermanEnv` и модель из `Stable-Baselines3` (например, `PPO`).
2.  **Обучение**: Запускается метод `model.learn()`, который итерирует по эпизодам:
    *   Агент выполняет действия в среде.
    *   Среда возвращает результат (новое состояние, вознаграждение).
    *   Агент использует эту информацию для обновления своей внутренней политики (нейронной сети), чтобы в будущем выбирать более "выгодные" действия.
3.  **Логирование**: Метрики обучения (среднее вознаграждение, длина эпизода и т.д.) автоматически логируются. Эти логи можно просматривать с помощью TensorBoard.
4.  **Сохранение**: Периодически (например, каждые N шагов) обученная модель сохраняется в файл с помощью `ModelManager`.

## Управление моделями (`ModelManager`)

*   **Сохранение**: Сохраняет обученную модель в директорию, указанную в `MODELS_PATH` (`/app/ai_models` в контейнере). Имя файла обычно включает название модели и количество шагов обучения, например `ppo_bomberman_1000000_steps.zip`.
*   **Загрузка**: При старте сервиса или по команде, `ModelManager` может загрузить ранее сохраненную модель, чтобы использовать ее для управления юнитами в игре (inference).

## Мониторинг с TensorBoard

Логи для TensorBoard сохраняются в директорию, указанную в `LOGS_PATH` (`/app/ai_logs` в контейнере).

Для просмотра прогресса тренировки:
1.  Убедитесь, что директория с логами (`infra/ai_logs` на хосте) проброшена в контейнер или доступна локально.
2.  Запустите TensorBoard:
    ```bash
    tensorboard --logdir=./infra/ai_logs --host 0.0.0.0 --port 6006
    ```
3.  Откройте в браузере `http://localhost:6006`.
